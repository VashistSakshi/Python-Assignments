{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is K-Nearest Neighbors (KNN) and how does it work in classification and regression?\n",
        "\n",
        "\n",
        "**Answer:** K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning algorithm.\n",
        "\n",
        "It does not learn a model\n",
        "\n",
        "It stores all training data\n",
        "\n",
        "Predictions are made only when a new data point arrives\n",
        "\n",
        "**How KNN works**\n",
        "\n",
        "Choose a value of K (number of neighbors)\n",
        "\n",
        "Compute distance between the new point and all training points\n",
        "\n",
        "Select the K closest points\n",
        "\n",
        "Aggregate their outputs\n",
        "\n",
        "**KNN for Classification**\n",
        "\n",
        "Uses majority voting\n",
        "\n",
        "The most common class among neighbors becomes the prediction\n",
        "\n",
        "Example:\n",
        "If K = 5 and neighbors are [Class A, A, B, A, B] → Prediction = Class A\n",
        "\n",
        "**KNN for Regression**\n",
        "\n",
        "Uses average (or weighted average) of neighbors’ values\n",
        "\n",
        "Example:\n",
        "Neighbors’ values = [10, 12, 14] → Prediction = 12"
      ],
      "metadata": {
        "id": "q41H5j8yKowB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the Curse of Dimensionality and how does it affect KNN?\n",
        "Meaning\n",
        "\n",
        "**Answer**:As the number of features (dimensions) increases:\n",
        "\n",
        "Data becomes sparse\n",
        "\n",
        "Distances between points become less meaningful\n",
        "\n",
        "**Why this hurts KNN**\n",
        "\n",
        "KNN relies completely on distance\n",
        "\n",
        "In high dimensions:\n",
        "\n",
        "Nearest and farthest neighbors become almost equally distant\n",
        "\n",
        "Noise dominates meaningful patterns\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Poor accuracy\n",
        "\n",
        "High computation cost\n",
        "\n",
        "Overfitting"
      ],
      "metadata": {
        "id": "xS-WvmkWLriu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is PCA? How is it different from feature selection?\n",
        "What is PCA?\n",
        "\n",
        "**Answer**: Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that:\n",
        "\n",
        "Converts original features into new orthogonal features\n",
        "\n",
        "These new features are called principal components\n",
        "\n",
        "Captures maximum variance with fewer dimensions\n",
        "\n",
        "**How PCA works (intuition)**\n",
        "\n",
        "Finds directions where data varies the most\n",
        "\n",
        "Projects data onto those directions\n",
        "\n",
        "Removes redundant information\n",
        "\n",
        "Difference from Feature Selection\n",
        "\n",
        "PCA creates new features\n",
        "\n",
        "Feature selection keeps original features\n",
        "\n",
        "PCA may lose interpretability but improves efficiency"
      ],
      "metadata": {
        "id": "eYclGsY1L3IQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: What are eigenvalues and eigenvectors in PCA and why are they important?\n",
        "Eigenvectors\n",
        "\n",
        "**Answer:**            \n",
        "Directions of maximum variance\n",
        "\n",
        "Become the principal components\n",
        "\n",
        "Eigenvalues\n",
        "\n",
        "Amount of variance captured by each eigenvector\n",
        "\n",
        "**Why they matter**\n",
        "\n",
        "Larger eigenvalue → more information\n",
        "\n",
        "PCA keeps components with highest eigenvalues\n",
        "\n",
        "Helps decide how many components to retain"
      ],
      "metadata": {
        "id": "1bqpi8R9L-0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** How do KNN and PCA complement each other in a pipeline?\n",
        "Problem with KNN alone\n",
        "\n",
        "**Answer**:\n",
        "Sensitive to noise\n",
        "\n",
        "Slow in high dimensions\n",
        "\n",
        "Suffers from curse of dimensionality\n",
        "\n",
        "**How PCA helps**\n",
        "\n",
        "Reduces dimensions\n",
        "\n",
        "Removes correlated features\n",
        "\n",
        "Improves distance reliability\n",
        "\n",
        "Combined Pipeline\n",
        "\n",
        "Scale features\n",
        "\n",
        "**Apply PCA**\n",
        "\n",
        "Train KNN on reduced data\n",
        "\n",
        " Result: Faster, more accurate, and more stable model"
      ],
      "metadata": {
        "id": "AQ0DEJxZMClj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**: KNN on Wine dataset with and without feature scaling"
      ],
      "metadata": {
        "id": "vlu2GXtFMHF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n",
        "\n",
        "OUTPUT:\n",
        "Accuracy without scaling: 0.7222\n",
        "Accuracy with scaling: 0.9722\n"
      ],
      "metadata": {
        "id": "-o3pE4s4MIrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**: PCA explained variance ratio (Wine dataset)\n"
      ],
      "metadata": {
        "id": "sbStqnhgMRe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "print(\"Explained variance ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "[0.36, 0.19, 0.11, 0.07, 0.06, ...]\n"
      ],
      "metadata": {
        "id": "ntxc3YnOMTtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**: KNN on PCA-transformed dataset (top 2 components)"
      ],
      "metadata": {
        "id": "WGAaZfcBMYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "print(\"Accuracy with PCA (2 components):\", accuracy_score(y_test, y_pred_pca))\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Accuracy with PCA (2 components): 0.9444\n"
      ],
      "metadata": {
        "id": "k4g3Ya51MZ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**: KNN with different distance metrics"
      ],
      "metadata": {
        "id": "X0H8LnYEMdpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Euclidean accuracy:\",\n",
        "      accuracy_score(y_test, knn_euclidean.predict(X_test_scaled)))\n",
        "\n",
        "print(\"Manhattan accuracy:\",\n",
        "      accuracy_score(y_test, knn_manhattan.predict(X_test_scaled)))\n",
        "\n",
        "OUTPUT:\n",
        "Euclidean accuracy: 0.9722\n",
        "Manhattan accuracy: 0.9444\n"
      ],
      "metadata": {
        "id": "FhQJyDarMfLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** High-dimensional gene expression dataset – Complete Strategy             \n",
        "**Answer:**                    \n",
        "**Step 1**: Use PCA\n",
        "\n",
        "Scale data\n",
        "\n",
        "Apply PCA to remove noise and correlation\n",
        "\n",
        "Reduce thousands of genes to meaningful components\n",
        "\n",
        "**Step 2**: Decide number of components\n",
        "\n",
        "Use explained variance (retain ~95%)\n",
        "\n",
        "Scree plot elbow method\n",
        "\n",
        "Balance performance and complexity\n",
        "\n",
        "**Step 3**: Apply KNN\n",
        "\n",
        "Train KNN on PCA-reduced data\n",
        "\n",
        "Choose optimal K using cross-validation\n",
        "\n",
        "**Step 4**: Evaluate the model\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision & recall\n",
        "\n",
        "Cross-validation stability\n",
        "\n",
        "Confusion matrix                \n",
        "**Step 5**: Stakeholder justification\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Handles small-sample, high-feature data\n",
        "\n",
        "Improves interpretability at system level\n",
        "\n",
        "Computationally efficient\n",
        "\n",
        "Widely accepted in biomedical research"
      ],
      "metadata": {
        "id": "bO46HXSYMm_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=7))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Pipeline accuracy:\", pipeline.score(X_test, y_test))\n",
        "\n",
        "OUTPUT:\n",
        "Pipeline accuracy: 0.91\n"
      ],
      "metadata": {
        "id": "L_N3HBWoMrrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}