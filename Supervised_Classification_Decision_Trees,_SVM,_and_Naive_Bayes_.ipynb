{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },  
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Answer**:\n",
        "Information Gain is a metric used in Decision Trees to decide which feature should be used to split the data at each node. It measures how much uncertainty (entropy) is reduced after splitting the dataset based on a feature.\n",
        "\n",
        "It is based on Entropy, which measures randomness in the data.\n",
        "\n",
        "The feature with the highest Information Gain is chosen for splitting.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "−\n",
        "∑\n",
        "(\n",
        "Samples in child/\n",
        "Samples in parent\n",
        "×\n",
        "Entropy(child)\n",
        ")\n",
        "\n",
        "\n",
        "**Usage** in Decision Trees:\n",
        "\n",
        "Commonly used in ID3 and C4.5 algorithms.\n",
        "\n",
        "Helps create more pure child nodes after each split."
      ],
      "metadata": {
        "id": "7r4_s3qJNfSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "**Answer**:\n",
        "Gini Impurity and Entropy are both measures used in Decision Trees to determine how well a feature splits the data, but they differ in how they calculate impurity and how they are used.\n",
        "\n",
        "Gini Impurity measures the probability that a randomly selected data point would be incorrectly classified if it were labeled according to the class distribution of the node. It is simpler to compute because it does not involve logarithmic calculations, which makes it faster and more suitable for large datasets. Gini is commonly used in the CART (Classification and Regression Trees) algorithm.\n",
        "\n",
        "Entropy measures the level of randomness or disorder in the data using a logarithmic function. It is more sensitive to changes in class probabilities and provides a more detailed measure of impurity. Because of this, entropy is often preferred when the dataset is smaller or when a more precise split is required. Entropy is mainly used in ID3 and C4.5 algorithms.\n",
        "\n",
        "In summary, Gini Impurity is computationally efficient and widely used in practice, while Entropy provides a more mathematically detailed measure of impurity but is slower to compute.\n",
        "\n"
      ],
      "metadata": {
        "id": "V5-dUNOGOXk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Answer**:\n",
        "Pre-Pruning is a technique used to stop the growth of a decision tree early to prevent overfitting.\n",
        "\n",
        "Common Pre-Pruning Methods:\n",
        "\n",
        "Limiting tree depth (max_depth)\n",
        "\n",
        "Minimum samples required to split (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Maximum number of leaf nodes\n",
        "\n",
        "**Benefits**:\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Improves generalization\n",
        "\n",
        "Faster training"
      ],
      "metadata": {
        "id": "nSwwzRTcOgDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Decision Tree using Gini Impurity (Practical)"
      ],
      "metadata": {
        "id": "Vnjzr0s_Oxj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "id": "O8T07kxoOzvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Answer**:\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "It finds an optimal hyperplane that maximizes the margin between classes.\n",
        "\n",
        "Uses support vectors, which are the closest data points to the decision boundary.\n",
        "\n",
        "Effective in high-dimensional spaces."
      ],
      "metadata": {
        "id": "CpUV9vVgO2Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**: What is the Kernel Trick in SVM?\n",
        "\n",
        "**Answer**:\n",
        "The Kernel Trick allows SVMs to solve non-linear problems by transforming data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "**Common Kernels**:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Radial Basis Function)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Benefit:\n",
        "Efficiently handles complex decision boundaries."
      ],
      "metadata": {
        "id": "nDtv-eoTO75m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: SVM with Linear and RBF Kernels (Practical)"
      ],
      "metadata": {
        "id": "hyppPnQ-PCqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, y_pred_linear))\n",
        "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, y_pred_rbf))\n"
      ],
      "metadata": {
        "id": "jVRFN0qUPEO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "**Answer**:\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "\n",
        "It is called “Naïve” because it assumes that all features are independent, which is rarely true in real-world data.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and fast\n",
        "\n",
        "Performs well on large datasets\n",
        "\n",
        "Works well for text classification"
      ],
      "metadata": {
        "id": "j8DQyImAPG46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "**Answer**:\n",
        "Gaussian Naïve Bayes is used when the features are continuous and follow a normal (Gaussian) distribution. It assumes that each feature value is drawn from a Gaussian distribution for each class. This variant is commonly applied in medical, scientific, and numerical datasets where feature values are real numbers.\n",
        "\n",
        "Multinomial Naïve Bayes is suitable for discrete count-based data. It is widely used in text classification problems such as spam detection and document categorization, where features represent word counts or term frequencies. It works well when the data consists of non-negative integer values.\n",
        "\n",
        "Bernoulli Naïve Bayes is designed for binary or boolean features. Instead of counting how many times a feature appears, it only considers whether a feature is present or absent. This makes it useful for applications like spam filtering where the presence or absence of a word matters more than its frequency.\n",
        "\n",
        "In summary, Gaussian Naïve Bayes works best with continuous data, Multinomial Naïve Bayes is ideal for count-based features, and Bernoulli Naïve Bayes is best suited for binary-valued features."
      ],
      "metadata": {
        "id": "tgfZYukaPYmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Gaussian Naïve Bayes on Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "VhUbwv08PNp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "JG5xzE1PPPuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
