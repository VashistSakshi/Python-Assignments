{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 ‚Äî Difference between AI, ML, DL, and Data Science (brief)\n",
        "\n",
        "AI (Artificial Intelligence)\n",
        "Scope: Broad field that builds systems that perform tasks normally requiring human intelligence (planning, reasoning, perception, language).\n",
        "Techniques: Rule-based systems, search, logic, planning, ML.\n",
        "Applications: Chatbots, game playing, automated planning, expert systems.\n",
        "\n",
        "ML (Machine Learning)\n",
        "Scope: Subset of AI focused on algorithms that learn patterns from data and make predictions/decisions.\n",
        "Techniques: Supervised (classification/regression), unsupervised (clustering), reinforcement learning.\n",
        "Applications: Spam detection, recommendation engines, fraud detection.\n",
        "\n",
        "DL (Deep Learning)\n",
        "Scope: Subset of ML using multi-layer neural networks (deep neural nets) to learn hierarchical features from large data.\n",
        "Techniques: CNNs, RNNs/LSTMs, Transformers, autoencoders.\n",
        "Applications: Image recognition, speech-to-text, large language models.\n",
        "\n",
        "Data Science\n",
        "Scope: Interdisciplinary field using statistics, ML, domain knowledge, and data engineering to extract insights and build data products.\n",
        "Techniques: Data cleaning, EDA, visualization, statistical inference, ML modeling, deployment/monitoring.\n",
        "Applications: Business analytics, A/B testing, forecasting, dashboards.\n",
        "\n",
        "Short comparison: AI is the broad goal; ML is the data-driven approach within AI; DL is a set of powerful ML models (neural nets) for complex data; Data Science is the practice of analyzing data (may use ML/DL) to answer questions and make decisions."
      ],
      "metadata": {
        "id": "5sD87c10VkDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 ‚Äî Overfitting and Underfitting (detect & prevent)\n",
        "\n",
        "Definitions\n",
        "\n",
        "Underfitting: Model is too simple to capture underlying patterns ‚Üí poor performance on train and test.\n",
        "\n",
        "Overfitting: Model fits training data (including noise) too closely ‚Üí low train error but high test error.\n",
        "\n",
        "Bias‚ÄìVariance tradeoff\n",
        "\n",
        "Underfitting ‚á® high bias, low variance.\n",
        "\n",
        "Overfitting ‚á® low bias, high variance.\n",
        "Goal: find sweet spot minimizing generalization error (bias¬≤ + variance + noise).\n",
        "\n",
        "Detection\n",
        "\n",
        "Compare training vs validation/test error. Underfitting ‚Üí both errors high. Overfitting ‚Üí training error much lower than validation error.\n",
        "\n",
        "Learning curves (plot error vs training set size): overfitting shows large gap between train/val; underfitting shows both high and close.\n",
        "\n",
        "Prevention / fixes\n",
        "\n",
        "For overfitting:\n",
        "\n",
        "More data (if possible).\n",
        "\n",
        "Regularization (L1/L2), dropout for NNs.\n",
        "\n",
        "Simpler model (reduce parameters/features).\n",
        "\n",
        "Early stopping (stop training when validation loss increases).\n",
        "\n",
        "Cross-validation to pick hyperparameters.\n",
        "\n",
        "For underfitting:\n",
        "\n",
        "Increase model complexity (add features, deeper model).\n",
        "\n",
        "Reduce regularization.\n",
        "\n",
        "Feature engineering.\n",
        "\n",
        "Cross-validation (k-fold) helps evaluate generalization reliably and tune hyperparameters to avoid overfitting."
      ],
      "metadata": {
        "id": "3QiGvAYjV_sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 ‚Äî Handling missing values (‚â•3 methods, examples)\n",
        "\n",
        "Deletion (listwise / row removal)\n",
        "\n",
        "Remove rows with missing values (or columns with many missing).\n",
        "\n",
        "Use when missingness is small and random (MCAR).\n",
        "\n",
        "Example: df = df.dropna(subset=['Age', 'Salary'])\n",
        "\n",
        "Imputation with summary statistics (mean/median/mode)\n",
        "\n",
        "Numeric: mean or median (median robust to outliers).\n",
        "\n",
        "Categorical: mode (most frequent).\n",
        "\n",
        "Example:\n",
        "\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['City'].fillna(df['City'].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "Predictive modeling (regression / KNN imputation / MICE)\n",
        "\n",
        "Fit model to predict missing values using other features (e.g., regress Age on other columns).\n",
        "\n",
        "KNN imputer uses nearest neighbors to impute. MICE (multiple imputation by chained equations) creates multiple plausible imputed datasets.\n",
        "\n",
        "Example: from sklearn.impute import KNNImputer then imputer.fit_transform(X)\n",
        "\n",
        "Indicator for missingness (helpful when missingness itself is informative)\n",
        "\n",
        "Create boolean column Age_missing = df['Age'].isnull().astype(int) and then impute Age (with median). This preserves signal that value was missing.\n",
        "\n",
        "Which to choose? Depends on cause of missingness (MCAR, MAR, MNAR), proportion missing, and importance of variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uot4Ym_oWJMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 ‚Äî Imbalanced dataset & two techniques\n",
        "\n",
        "Imbalanced dataset: Class distribution heavily skewed (e.g., 95% negative, 5% positive). Standard classifiers may be biased toward majority class.\n",
        "\n",
        "Techniques\n",
        "\n",
        "Resampling\n",
        "\n",
        "Oversampling (increase minority class): random oversampling or SMOTE (Synthetic Minority Over-sampling Technique) ‚Äî SMOTE creates synthetic minority examples by interpolating neighbors. Practical: imblearn.over_sampling.SMOTE()\n",
        "\n",
        "Undersampling (reduce majority class): random undersampling or more advanced methods (NearMiss). Practical: imblearn.under_sampling.RandomUnderSampler()\n",
        "\n",
        "Pros/cons: oversampling can overfit, undersampling may lose information; SMOTE reduces overfitting risk vs naive oversampling.\n",
        "\n",
        "Algorithmic approaches / class weighting\n",
        "\n",
        "Use class weights in loss function or model APIs (e.g., class_weight='balanced' in sklearn classifiers, or set pos_weight in XGBoost). Penalizes misclassification of minority class more heavily.\n",
        "\n",
        "Good when you want to keep original data distribution and avoid synthetic data."
      ],
      "metadata": {
        "id": "0x42xls7WRUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 ‚Äî Feature scaling importance; Min‚ÄìMax vs Standardization\n",
        "\n",
        "Why scale?\n",
        "\n",
        "Many ML algorithms rely on distance or gradient magnitudes (KNN, SVM, k-means, PCA, neural networks). Features on different scales distort distances and slow/unstable training (gradient descent). Scaling makes features comparable.\n",
        "\n",
        "Min‚ÄìMax scaling (Normalization)\n",
        "\n",
        "Formula:\n",
        "ùë•\n",
        "‚Ä≤\n",
        "=\n",
        "(\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "ùëö\n",
        "ùëñ\n",
        "ùëõ\n",
        ")\n",
        "/\n",
        "(\n",
        "ùë•\n",
        "ùëö\n",
        "ùëé\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "ùëö\n",
        "ùëñ\n",
        "ùëõ\n",
        ")\n",
        "x\n",
        "‚Ä≤\n",
        "=(x‚àíx\n",
        "min\n",
        "\t‚Äã\n",
        "\n",
        ")/(x\n",
        "max\n",
        "\t‚Äã\n",
        "\n",
        "‚àíx\n",
        "min\n",
        "\t‚Äã\n",
        "\n",
        ") ‚Üí maps to [0,1] (or other range).\n",
        "\n",
        "Pros: preserves shape of original distribution, bounded range.\n",
        "\n",
        "Cons: sensitive to outliers (min/max changes if outliers present).\n",
        "\n",
        "Standardization (Z-score)\n",
        "\n",
        "Pros: less sensitive to outliers than min‚Äìmax (but still affected), good for algorithms assuming zero-centered data (e.g., logistic regression, PCA).\n",
        "\n",
        "Use when inputs approximate Gaussian or when using regularized models.\n",
        "\n",
        "Impact examples\n",
        "\n",
        "KNN/SVM: both sensitive to feature scales ‚Äî always scale.\n",
        "\n",
        "Gradient descent: standardization often improves convergence speed."
      ],
      "metadata": {
        "id": "fm6A-KVsWcr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 ‚Äî Label Encoding vs One-Hot Encoding (comparison & when to use)\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Maps categories to integers (e.g., A‚Üí0, B‚Üí1, C‚Üí2).\n",
        "\n",
        "Use when categorical variable is ordinal (there is a natural ordering, e.g., low, medium, high) so numeric mapping preserves order.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Creates binary columns for each category (e.g., is_A, is_B, is_C).\n",
        "\n",
        "Use for nominal variables (no natural order). Avoid implying ordinal relationships.\n",
        "\n",
        "Cons: increases dimensionality (use sparse representation if many categories). For high-cardinality, consider target encoding or embedding.\n",
        "\n",
        "When prefer one over other\n",
        "\n",
        "Ordinal categorical ‚Üí Label Encoding.\n",
        "\n",
        "Nominal categorical and models expecting numeric input (tree-based models can sometimes handle label encoding but one-hot is safer for linear models) ‚Üí One-Hot Encoding.\n",
        "\n",
        "For tree-based models (random forest, XGBoost), label encoding can work OK, but if categories have no order, many practitioners still prefer one-hot or target encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "lrZbcNzWWiYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 ‚Äî Google Play Store: Category vs Rating (code + expected output)"
      ],
      "metadata": {
        "id": "8J8SJH08Wm4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: Google Play Store - category vs rating\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/googleplaystore.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Clean rating\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Average rating by category\n",
        "cat_rating = df.groupby('Category', observed=True)['Rating'].mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"Top 10 categories by average rating:\")\n",
        "print(cat_rating.head(10).round(3))\n",
        "\n",
        "print(\"\\nBottom 10 categories by average rating:\")\n",
        "print(cat_rating.tail(10).round(3))\n",
        "\n",
        "# Plot (top 10)\n",
        "top10 = cat_rating.head(10)\n",
        "plt.figure(figsize=(10,5))\n",
        "top10.plot(kind='bar')\n",
        "plt.title(\"Top 10 Categories by Average Rating\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4hjUJxS8Wqe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 ‚Äî Titanic: survival by Pclass and Age groups (code + expected output)"
      ],
      "metadata": {
        "id": "qYgzgBQQWt8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8: Titanic dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/titanic.csv\"\n",
        "t = pd.read_csv(url)\n",
        "\n",
        "# Ensure types\n",
        "t['Survived'] = pd.to_numeric(t['Survived'], errors='coerce')\n",
        "t['Pclass'] = pd.to_numeric(t['Pclass'], errors='coerce')\n",
        "t['Age'] = pd.to_numeric(t['Age'], errors='coerce')\n",
        "\n",
        "# a) Survival rate by Pclass\n",
        "survival_by_pclass = t.groupby('Pclass')['Survived'].mean().sort_index()\n",
        "print(\"Survival rate by Pclass (fraction):\")\n",
        "print((survival_by_pclass * 100).round(2))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "survival_by_pclass.plot(kind='bar')\n",
        "plt.title(\"Survival Rate by Pclass\")\n",
        "plt.ylabel(\"Survival rate\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# b) Age effect: children (<18) vs adults (>=18)\n",
        "t['AgeGroup'] = np.where(t['Age'] < 18, 'Child', 'Adult')\n",
        "survival_by_agegroup = t.groupby('AgeGroup')['Survived'].mean()\n",
        "print(\"\\nSurvival rate by Age group:\")\n",
        "print((survival_by_agegroup * 100).round(2))\n",
        "\n",
        "print(\"\\nCounts:\")\n",
        "print(t['AgeGroup'].value_counts())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "survival_by_agegroup.plot(kind='bar')\n",
        "plt.title(\"Survival Rate: Children vs Adults\")\n",
        "plt.ylabel(\"Survival rate\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-huKUdj0WwU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9 ‚Äî Flight Price Prediction: days left and airline comparison (code + expected output)"
      ],
      "metadata": {
        "id": "YMnrrdHIW0Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: Flight price analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv\"\n",
        "f = pd.read_csv(url)\n",
        "\n",
        "print(\"Columns:\", list(f.columns))\n",
        "# Attempt to locate a days-left column\n",
        "candidates = [c for c in f.columns if any(k in c.lower() for k in ['day','days','left','dep','departure'])]\n",
        "print(\"Candidates for days-like columns:\", candidates)\n",
        "\n",
        "# Example common column names: 'days_left' or 'days' or 'Days_left'\n",
        "# If dataset has a column e.g. 'days_left' (numeric):\n",
        "days_col = None\n",
        "for c in candidates:\n",
        "    if pd.api.types.is_numeric_dtype(f[c]):\n",
        "        days_col = c\n",
        "        break\n",
        "\n",
        "if days_col is None:\n",
        "    # If there is a departure date and booking date, compute days difference\n",
        "    possible_date_cols = [c for c in f.columns if 'date' in c.lower()]\n",
        "    print(\"No direct numeric days-left found. Date columns:\", possible_date_cols)\n",
        "else:\n",
        "    # Relationship between days left and price: compute median price per days-left\n",
        "    med_price = f.groupby(days_col)['Price'].median().reset_index().sort_values(by=days_col)\n",
        "    print(med_price.head(10))\n",
        "    # Plot median price vs days-left\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(med_price[days_col], med_price['Price'], marker='o')\n",
        "    plt.gca().invert_xaxis()  # usually to show days->near departure\n",
        "    plt.title(\"Median Price vs Days Left\")\n",
        "    plt.xlabel(\"Days left until departure\")\n",
        "    plt.ylabel(\"Median Price\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# b) Compare airlines for a route (e.g., 'Delhi' and 'Mumbai' appear in route columns)\n",
        "# Find route-like column or source & destination columns:\n",
        "route_cols = [c for c in f.columns if any(k in c.lower() for k in ['route','from','to','source','destination'])]\n",
        "print(\"Potential route columns:\", route_cols)\n",
        "\n",
        "# Filter rows containing both Delhi and Mumbai somewhere\n",
        "mask = f.apply(lambda r: r.astype(str).str.contains('Delhi', case=False, na=False).any() and r.astype(str).str.contains('Mumbai', case=False, na=False).any(), axis=1)\n",
        "if mask.any():\n",
        "    dm = f[mask]\n",
        "    print(\"Records for Delhi-Mumbai:\", len(dm))\n",
        "    # find airline column\n",
        "    airline_col = next((c for c in f.columns if 'air' in c.lower()), None)\n",
        "    if airline_col:\n",
        "        median_by_air = dm.groupby(airline_col)['Price'].median().sort_values()\n",
        "        print(\"Median price by airline for Delhi-Mumbai:\\n\", median_by_air)\n",
        "        median_by_air.plot(kind='bar', figsize=(8,4))\n",
        "        plt.title(\"Median Price by Airline for Delhi-Mumbai\")\n",
        "        plt.ylabel(\"Median Price\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No airline column detected.\")\n",
        "else:\n",
        "    print(\"No explicit Delhi-Mumbai rows found. Inspect route columns manually.\")\n"
      ],
      "metadata": {
        "id": "QF08VlFEW1xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 ‚Äî HR Analytics: attrition drivers & projects relation (code + expected output)"
      ],
      "metadata": {
        "id": "shMyroq4W5IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: HR analytics - correlations & projects vs attrition\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/hr_analytics.csv\"\n",
        "h = pd.read_csv(url)\n",
        "\n",
        "print(\"Columns:\", list(h.columns))\n",
        "\n",
        "# Normalize attrition to binary\n",
        "if 'Attrition' in h.columns:\n",
        "    h['AttritionB'] = h['Attrition'].map({'Yes':1,'No':0})\n",
        "else:\n",
        "    # fallback: if first column is Attrition\n",
        "    h['AttritionB'] = (h.iloc[:,0].astype(str).str.lower() == 'yes').astype(int)\n",
        "\n",
        "# Numeric correlations\n",
        "numeric = h.select_dtypes(include=[np.number]).drop(columns=['AttritionB'], errors='ignore')\n",
        "corr_with_attr = numeric.corrwith(h['AttritionB']).abs().sort_values(ascending=False)\n",
        "print(\"Top features correlated with Attrition:\")\n",
        "print(corr_with_attr.head(10).round(3))\n",
        "\n",
        "# Visualize a few important factors if present\n",
        "for col in ['JobSatisfaction','OverTime','MonthlyIncome','TotalWorkingYears','DistanceFromHome','WorkLifeBalance']:\n",
        "    if col in h.columns:\n",
        "        if h[col].dtype == 'object':\n",
        "            agg = h.groupby(col)['AttritionB'].mean().sort_values(ascending=False)\n",
        "            print(f\"\\nAttrition rate by {col}:\")\n",
        "            print((agg*100).round(2))\n",
        "            agg.plot(kind='bar', figsize=(6,3))\n",
        "            plt.title(f\"Attrition rate by {col}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            # bin numeric column and show attrition rate\n",
        "            bins = pd.qcut(h[col].rank(method='first'), q=5, duplicates='drop')\n",
        "            agg = h.groupby(bins)['AttritionB'].mean()\n",
        "            print(f\"\\nAttrition by binned {col}:\")\n",
        "            print((agg*100).round(2))\n",
        "            agg.plot(kind='bar', figsize=(6,3))\n",
        "            plt.title(f\"Attrition by binned {col}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# b) Are employees with more projects more likely to leave?\n",
        "proj_candidates = [c for c in h.columns if 'project' in c.lower() or 'numproject' in c.lower() or 'no_of_projects' in c.lower() or 'num_projects' in c.lower()]\n",
        "print(\"Project-like columns detected:\", proj_candidates)\n",
        "if proj_candidates:\n",
        "    pc = proj_candidates[0]\n",
        "    grp = h.groupby(pc)['AttritionB'].mean().sort_index()\n",
        "    print(\"\\nAttrition rate by # projects:\")\n",
        "    print((grp*100).round(2))\n",
        "    grp.plot(kind='bar', figsize=(6,3))\n",
        "    plt.title(\"Attrition rate by Number of Projects\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No explicit project-count column; check dataset for similar fields (e.g., 'NumProjects' or 'No_of_Projects').\")\n"
      ],
      "metadata": {
        "id": "Sjb5DgNuW8XI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}