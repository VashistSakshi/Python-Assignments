{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What is Ensemble Learning in Machine Learning? Explain the key idea behind it.\n",
        "\n",
        "**Answer**:\n",
        "Ensemble Learning is a machine learning technique in which multiple models (called base learners) are trained and combined to solve the same problem. The key idea is that a group of models working together can achieve better performance than a single model.\n",
        "\n",
        "Each model may make different errors, and by combining their predictions (through voting or averaging), ensemble methods reduce variance, bias, or both. This leads to improved accuracy, robustness, and generalization on unseen data.\n",
        "\n",
        "Common ensemble methods include Bagging, Boosting, and Random Forests."
      ],
      "metadata": {
        "id": "IUCFD72yUdnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: What is the difference between Bagging and Boosting?\n",
        "\n",
        "**Answer**:\n",
        "Bagging (Bootstrap Aggregating) and Boosting are both ensemble techniques, but they differ in how models are trained and combined. Bagging trains multiple models independently on different bootstrap samples of the dataset and combines their predictions equally, mainly to reduce variance and prevent overfitting. Random Forest is a popular example of Bagging.\n",
        "\n",
        "Boosting, on the other hand, trains models sequentially. Each new model focuses more on the data points that were misclassified by previous models. The goal of Boosting is to reduce bias and improve overall performance. Algorithms like AdaBoost and Gradient Boosting follow this approach.\n",
        "\n",
        "In summary, Bagging focuses on variance reduction using independent models, while Boosting focuses on bias reduction using dependent, sequential models."
      ],
      "metadata": {
        "id": "9YAwCK4rUux5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "**Answer**:\n",
        "Bootstrap sampling is a technique where multiple training datasets are created by randomly sampling data points from the original dataset with replacement. This means some samples may appear multiple times while others may not appear at all.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling ensures that each base model (decision tree) is trained on a slightly different dataset. This introduces diversity among the models, which helps reduce variance and improves the stability and accuracy of the ensemble."
      ],
      "metadata": {
        "id": "LlNrXt8gUzY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "**Answer**:\n",
        "Out-of-Bag (OOB) samples are the data points that are not selected during bootstrap sampling for a particular model. On average, about 36% of the original data becomes OOB for each tree.\n",
        "\n",
        "The OOB score is calculated by evaluating each data point using only the models for which that point was not included in training. This provides an unbiased estimate of model performance without needing a separate validation set, making OOB a useful and efficient evaluation method for Bagging and Random Forest models."
      ],
      "metadata": {
        "id": "xGe3KVRRU4CO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "**Answer**:\n",
        "In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (Gini or Entropy) at each split. However, since a single tree is prone to overfitting, its feature importance can be unstable and sensitive to noise.\n",
        "\n",
        "In a Random Forest, feature importance is averaged across many decision trees. This makes the importance scores more reliable, stable, and robust. Random Forest provides a better overall understanding of which features truly contribute to predictions compared to a single decision tree."
      ],
      "metadata": {
        "id": "KQ2h__OYU8q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**: Random Forest on Breast Cancer Dataset (Feature Importance)"
      ],
      "metadata": {
        "id": "EvOqAKFVVCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "1B6pT1P3VEfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**: Bagging Classifier vs Single Decision Tree (Iris Dataset)"
      ],
      "metadata": {
        "id": "nqbT6oV1VNwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))\n"
      ],
      "metadata": {
        "id": "qSZSbbmxVQl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**: Random Forest with Hyperparameter Tuning (GridSearchCV)"
      ],
      "metadata": {
        "id": "fwsxXpy1VUv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearch\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hGJs7cHtVXSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**: Bagging Regressor vs Random Forest Regressor (California Housing)"
      ],
      "metadata": {
        "id": "jOsjHmMPVbLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, bag_pred))\n",
        "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "id": "b7xCmPElVeZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**: Ensemble Learning for Loan Default Prediction (Real-World Case)\n",
        "\n",
        "**Answer**:\n",
        "To predict loan default, I would first analyze the dataset to understand feature types, class imbalance, and noise. Since financial data often has complex patterns, I would compare both Bagging and Boosting approaches. If the data shows high variance and overfitting, I would prefer Bagging methods like Random Forest. If bias is high and complex relationships exist, I would choose Boosting methods such as Gradient Boosting or XGBoost.\n",
        "\n",
        "To handle overfitting, I would use techniques like limiting tree depth, regularization, early stopping (for boosting), and cross-validation. The base models would typically be decision trees because they capture non-linear relationships well and work effectively in ensembles.\n",
        "\n",
        "For evaluation, I would use cross-validation along with metrics such as accuracy, precision, recall, F1-score, and AUC-ROC, especially since loan default prediction is a high-risk classification problem.\n",
        "\n",
        "Ensemble learning improves decision-making in this context by increasing prediction reliability, reducing model risk, and providing more stable and accurate results. This helps the financial institution make better lending decisions and minimize default risk.\n"
      ],
      "metadata": {
        "id": "OcTipXeAVpNA"
      }
    }
  ]
}