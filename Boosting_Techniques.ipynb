{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },  
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "**Answer**:\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners to create a strong predictive model. A weak learner is a model that performs only slightly better than random guessing.\n",
        "\n",
        "Boosting improves weak learners by training models sequentially. Each new model focuses more on the data points that were misclassified by previous models. By giving higher importance to difficult samples, boosting gradually reduces errors and improves overall model accuracy.\n",
        "\n",
        "Popular boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, and CatBoost.**bold text**"
      ],
      "metadata": {
        "id": "7_1qlQMJYY0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "**Answer**:\n",
        "AdaBoost trains models sequentially by adjusting the weights of misclassified samples. Incorrectly predicted samples receive higher weights so that subsequent models focus more on them. AdaBoost mainly uses simple models like decision stumps and combines them using weighted voting.\n",
        "\n",
        "Gradient Boosting, instead of reweighting samples, trains each new model to correct the errors (residuals) made by the previous model. It uses gradient descent optimization to minimize a loss function. Gradient Boosting is more flexible and can handle complex loss functions compared to AdaBoost."
      ],
      "metadata": {
        "id": "_cYpFL_eYc2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: How does regularization help in XGBoost?\n",
        "\n",
        "**Answer**:\n",
        "Regularization in XGBoost helps prevent overfitting by penalizing complex models. It includes both L1 (Lasso) and L2 (Ridge) regularization terms, which control tree complexity by limiting leaf weights and the number of splits.\n",
        "\n",
        "By adding regularization to the objective function, XGBoost ensures that trees remain simple and generalize better to unseen data. This makes XGBoost more robust and accurate, especially on noisy datasets."
      ],
      "metadata": {
        "id": "iRSByZNMYgcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "**Answer**:\n",
        "CatBoost is designed to handle categorical features directly without requiring manual encoding such as one-hot encoding. It uses a technique called ordered target encoding, which prevents data leakage and improves model performance.\n",
        "\n",
        "Additionally, CatBoost automatically handles missing values and reduces overfitting using symmetric trees. This makes it efficient, accurate, and easy to use for datasets with many categorical variables."
      ],
      "metadata": {
        "id": "-h-ETV7hYjg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "**Answer**:\n",
        "Boosting techniques are preferred in applications where high accuracy and complex patterns are required. Common real-world applications include credit risk prediction, fraud detection, medical diagnosis, recommendation systems, customer churn prediction, and ad click-through rate prediction.\n",
        "\n",
        "Boosting works better than bagging in scenarios with high bias, imbalanced data, and non-linear relationships."
      ],
      "metadata": {
        "id": "EfhShuhXYoTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**: AdaBoost Classifier on Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "9-Sa44EkYrr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train AdaBoost\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "zNtrc1s4YuI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**: Gradient Boosting Regressor on California Housing Dataset"
      ],
      "metadata": {
        "id": "lL6BWoI5YykC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# R2 score\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "KRuaIj_uY7bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** XGBoost Classifier with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Way0KbJeY_mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearch\n",
        "grid = GridSearchCV(xgb, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "99a5cyWwZC2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**: CatBoost Classifier with Confusion Matrix"
      ],
      "metadata": {
        "id": "7X1eyKcIZGNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n"
      ],
      "metadata": {
        "id": "-7d6e5Q7ZJAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**: Boosting-Based Data Science Pipeline for Loan Default Prediction\n",
        "\n",
        "**Answer**:\n",
        "First, I would clean the data by handling missing values using techniques such as mean/median imputation for numerical features and most-frequent or model-based imputation for categorical features. For categorical variables, I would prefer CatBoost since it handles them natively.\n",
        "\n",
        "Given the imbalanced nature of loan default data, I would apply techniques such as class weighting or SMOTE. Among boosting algorithms, I would compare XGBoost and CatBoost. XGBoost is powerful for numeric data and allows fine-grained control, while CatBoost is ideal if categorical features dominate.\n",
        "\n",
        "For hyperparameter tuning, I would use GridSearchCV or RandomizedSearchCV focusing on learning rate, depth, and number of estimators. To evaluate performance, I would use metrics like ROC-AUC, Precision-Recall, F1-score, and Recall, as false negatives (missed defaulters) are costly in finance.\n",
        "\n",
        "From a business perspective, boosting improves decision-making by providing accurate risk predictions, reducing loan defaults, improving profitability, and enabling fairer credit decisions."
      ],
      "metadata": {
        "id": "_OvVQKSTZPEh"
      }
    }
  ]
}
